{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed40d6e0",
   "metadata": {},
   "source": [
    "1. Write a python program which searches all the product under a particular product from www.amazon.in.\n",
    "The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search\n",
    "for guitars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e711c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bdf5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_prod_amazon(search_str):\n",
    "    #intiating the browser\n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe')\n",
    "    #loading the given url\n",
    "    driver.get('https://www.amazon.in/')\n",
    "    #Send input to search bar\n",
    "    time.sleep(5)\n",
    "    search_bar= WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//input[@name=\"field-keywords\"]')))\n",
    "    search_bar.send_keys(search_str)\n",
    "    driver.find_element_by_id(\"nav-search-submit-button\").click()\n",
    "    time.sleep(10)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "660d8b5c",
   "metadata": {},
   "source": [
    "2. In the above question, now scrape the following details of each product listed in first 3 pages of your\n",
    "search results and save it in a data frame and csv. In case if any product has less than 3 pages in search\n",
    "results then scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "“Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a914507",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prod_info(input_string):\n",
    "    driver=search_prod_amazon(input_string)\n",
    "    driver.maximize_window()    \n",
    "    list_items=[]\n",
    "    brand_list=[]\n",
    "    name_list=[]\n",
    "    price_list=[]\n",
    "    return_list=[]\n",
    "    exp_delvry=[]\n",
    "    availb_list=[]\n",
    "    prod_url=[]\n",
    "    ret_exchng_list=[]\n",
    "    \n",
    "    for i in range(1,4):\n",
    "        #WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.XPATH, '//div[@data-component-type=\"s-search-result\"]//a[@class=\"a-link-normal s-link-style a-text-normal\"]')))\n",
    "        list_items=driver.find_elements_by_xpath('//div[@data-component-type=\"s-search-result\"]')\n",
    "        for item in list_items:\n",
    "            try:\n",
    "                brand_list.append(item.find_element_by_xpath('.//span[@class=\"a-size-base-plus a-color-base\"]').text)\n",
    "            except:\n",
    "                brand_list.append('-')\n",
    "            try:\n",
    "                name_list.append(item.find_element_by_xpath('.//h2[1]//span').text)\n",
    "            except:\n",
    "                name_list.append('-')\n",
    "            try:\n",
    "                price_list.append(item.find_element_by_xpath('.//span[@class=\"a-price-whole\"]/parent::span[1]').text)\n",
    "            except:\n",
    "                price_list.append('-')\n",
    "            try:\n",
    "                exp_delvry.append(item.find_element_by_xpath('.//span[starts-with(.,\"Get it by\")]/span[2]').text)\n",
    "            except:\n",
    "                exp_delvry.append('-')\n",
    "            try:\n",
    "                availb_list.append(item.find_element_by_xpath('.//span[@class=\"a-size-base a-color-price\"]').text)\n",
    "            except:\n",
    "                availb_list.append('-')\n",
    "            try:\n",
    "                prod_url.append(item.find_element_by_xpath('.//a[@class=\"a-link-normal s-link-style a-text-normal\" or @class=\"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"]').get_attribute('href'))\n",
    "            except:\n",
    "                prod_url.append('-')\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        for urls in prod_url:\n",
    "            try:\n",
    "                driver.get(urls)\n",
    "                time.sleep(4)\n",
    "                ret_exchng_list.append(driver.find_element_by_xpath('//div[@data-name=\"RETURNS_POLICY\"]//a[@class=\"a-size-small a-link-normal a-text-normal\"]').text)\n",
    "            except:\n",
    "                ret_exchng_list.append('-')\n",
    "        # Closing new_url tab\n",
    "        driver.close()\n",
    "        # Switching to old tab\n",
    "        driver.switch_to.window(driver.window_handles[0])\n",
    "        try:\n",
    "            driver.find_element_by_xpath('//a[.=\"Next→\" or .=\"Next\"]').click()\n",
    "        except NoSuchElementException:\n",
    "            driver.refresh()\n",
    "            time.sleep(5)\n",
    "            driver.find_element_by_xpath('//a[.=\"Next→\" or .=\"Next\"]').click()\n",
    "    data_collection=pd.DataFrame({'Brand Name':brand_list,'Name of the Product':name_list,'Price':price_list,'Expected Delivery':exp_delvry,'Availability':availb_list,'Product URL':prod_url,'Return/Exchange':ret_exchng_list})\n",
    "    return data_collection\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_prod_info('Jerkins')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f2412",
   "metadata": {},
   "source": [
    "3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080ce9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def google_images(search_str_list):\n",
    "    #intiating the browser\n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe')\n",
    "    #loading the given url\n",
    "    driver.get('https://images.google.com/')\n",
    "    driver.maximize_window()\n",
    "    #Send input to search bar\n",
    "    time.sleep(5)\n",
    "    image_urls=[]\n",
    "    WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//input[@title=\"Search\"]')))\n",
    "    for key in search_str_list:\n",
    "        driver.find_element_by_xpath('//input[@title=\"Search\"]').send_keys(key)\n",
    "        driver.find_element_by_xpath('//button[@aria-label=\"Google Search\"]').click()\n",
    "        time.sleep(10)\n",
    "        #try:\n",
    "        images=driver.find_elements_by_xpath('//div[@class=\"islrc\"]/div/a[@jsname=\"sTFXNd\"]//img[1]')\n",
    "        print(len(images))\n",
    "        for image in images[:10]:\n",
    "            try:\n",
    "                image_urls.append(image.get_attribute('src').strip())\n",
    "            except:\n",
    "                image_urls.append('-')\n",
    "        driver.find_element_by_xpath('//input[@title=\"Search\"]').clear()\n",
    "        print(key,' is done.')\n",
    "    result=pd.DataFrame({'Image Urls':image_urls})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_keys=['fruits','cars','Machine Learning','Guitar','Cakes']\n",
    "google_images(search_keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83193753",
   "metadata": {},
   "source": [
    "4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on\n",
    "www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be\n",
    "scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”,\n",
    "“Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the\n",
    "details is missing then replace it by “- “. Save your results in a dataframe and CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37504828",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_prod_flipkart(search_str):\n",
    "    #intiating the browser\n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe')\n",
    "    #loading the given url\n",
    "    driver.get('https://www.flipkart.com/')\n",
    "    #Send input to search bar\n",
    "    time.sleep(5)\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "    search_bar= WebDriverWait(driver, 30).until(EC.presence_of_element_located((By.XPATH, '//input[@title=\"Search for products, brands and more\"]')))\n",
    "    driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys(search_str)\n",
    "    driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "    time.sleep(5)\n",
    "    brand_list=[]\n",
    "    name_list=[]\n",
    "    color_list=[]\n",
    "    ram_list=[]\n",
    "    rom_list=[]\n",
    "    p_cam_list=[]\n",
    "    s_cam_list=[]\n",
    "    disp_size_list=[]\n",
    "    btry_cap_list=[]\n",
    "    price_list=[]\n",
    "    url_list=[]\n",
    "    \n",
    "    list_item=driver.find_elements_by_xpath('//div[@class=\"_13oc-S\"]/div')\n",
    "    for item in list_item:\n",
    "        brand_list.append('-')\n",
    "        try:\n",
    "            name=item.find_element_by_xpath('.//div[@class=\"_4rR01T\"]').text\n",
    "            name_list.append(name)\n",
    "            color=name[name.index('(')+1:]\n",
    "            color=color[:color.index(',')]\n",
    "            color_list.append(color)\n",
    "        except:\n",
    "            name_list.append('-')\n",
    "            color_list.append('-')        \n",
    "        try:\n",
    "            ram_str=item.find_element_by_xpath('.//ul[@class=\"_1xgFaf\"]/li[contains(.,\"RAM\")]').text\n",
    "            ram=ram_str.split('|')[0]\n",
    "            rom=ram_str.split('|')[1]\n",
    "            ram_list.append(ram)\n",
    "            rom_list.append(rom)\n",
    "        except:\n",
    "            ram_list.append('-')\n",
    "            rom_list.append('-')\n",
    "        try:\n",
    "            cam_str=item.find_element_by_xpath('.//ul[@class=\"_1xgFaf\"]/li[contains(.,\"Camera\")]').text\n",
    "            f_cam=cam_str.split('|')[0]\n",
    "            s_cam=cam_str.split('|')[1]\n",
    "            p_cam_list.append(f_cam)\n",
    "            s_cam_list.append(s_cam)\n",
    "        except:\n",
    "            p_cam_list.append('-')\n",
    "            s_cam_list.append('-')\n",
    "        try:\n",
    "            dis_str=item.find_element_by_xpath('.//ul[@class=\"_1xgFaf\"]/li[contains(.,\"Display\")]').text\n",
    "            disp_size_list.append(dis_str)\n",
    "        except:\n",
    "            disp_size_list.append('-')\n",
    "        try:\n",
    "            btry=item.find_element_by_xpath('.//ul[@class=\"_1xgFaf\"]/li[contains(.,\"Battery\")]').text\n",
    "            btry_cap_list.append(btry)\n",
    "        except:\n",
    "            btry_cap_list.append('-')\n",
    "        try:\n",
    "            price=item.find_element_by_xpath('.//div[@class=\"_30jeq3 _1_WHN1\"]').text\n",
    "            price_list.append(price)\n",
    "        except:\n",
    "            price_list.append('-')\n",
    "        try:\n",
    "            url=item.find_element_by_xpath('.//a[1]').get_attribute('href')\n",
    "            url_list.append(url)\n",
    "        except:\n",
    "            url_list.append('-')\n",
    "    result=pd.DataFrame({'Brand':brand_list,'Name':name_list,'Color':color_list,'Price':price_list,'RAM':ram_list,'ROM':rom_list,'Battery':btry_cap_list,'Display':disp_size_list,'Primary Camera':p_cam_list,'Secondary Camera':s_cam_list,'Url':url_list})\n",
    "    result.to_csv('C:/Users/yn/Desktop/Yuvi/DataTrained/Internship/flipkartProdDetails.csv')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a806f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_prod_flipkart('Moto G5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7387be86",
   "metadata": {},
   "source": [
    "5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google\n",
    "maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e5673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lat_long_googlemaps(search_str):\n",
    "    #intiating the browser\n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe')\n",
    "    #loading the given url\n",
    "    driver.get('https://www.google.com/maps/')\n",
    "    #Send input to search bar\n",
    "    time.sleep(5)\n",
    "    driver.find_element_by_xpath('//input[@id=\"searchboxinput\"]').clear()\n",
    "    driver.find_element_by_xpath('//input[@id=\"searchboxinput\"]').send_keys(search_str)\n",
    "    driver.find_element_by_xpath('//button[@id=\"searchbox-searchbutton\"]').click()\n",
    "    time.sleep(4)\n",
    "    curl=driver.current_url\n",
    "    ptrn=re.compile('https://www.google.com/maps/place/.*/@(.*?)/')\n",
    "    res=re.search(ptrn,curl)\n",
    "    longitude=res.group(1).split(',')[1]\n",
    "    latitude=res.group(1).split(',')[0]\n",
    "    print(\"Longitude: \",longitude,\"Lattitude: \",latitude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f2611",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_lat_long_googlemaps('Bangalore,Karnataka')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f1f636",
   "metadata": {},
   "source": [
    "6. Write a program to scrap details of all the funding deals for second quarter (i.e Jan 21 – March 21)\n",
    "from trak.in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195e3b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_funding_deals():\n",
    "    #intiating the browser\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    \n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe',chrome_options=options)\n",
    "    #loading the given url\n",
    "    driver.get('https://trak.in/')\n",
    "    #Send input to search bar\n",
    "    time.sleep(5)\n",
    "    driver.maximize_window()\n",
    "    driver.find_element_by_xpath('(//a[.=\"Funding Deals\"])[1]').click()\n",
    "    time.sleep(5)\n",
    "    date_list=[]\n",
    "    startup_list=[]\n",
    "    industry_vertical_list=[]\n",
    "    sub_vertical_list=[]\n",
    "    city_location_list=[]\n",
    "    investors_name_list=[]\n",
    "    investment_type_list=[]\n",
    "    amount_list=[]\n",
    "    \n",
    "    try:\n",
    "        tables_list=driver.find_elements_by_xpath('//h2[.=\"January, 2021\" or .=\"February, 2021\" or .=\"March, 2021\"]/following-sibling::div[1]//div[@class=\"dataTables_scrollBody\"]/table[1]')\n",
    "    except:\n",
    "        driver.refresh()\n",
    "        time.sleep(6)\n",
    "        tables_list=driver.find_elements_by_xpath('//h2[.=\"January, 2021\" or .=\"February, 2021\" or .=\"March, 2021\"]/following-sibling::div[1]//div[@class=\"dataTables_scrollBody\"]/table[1]') \n",
    "    print(len(tables_list))\n",
    "    for table in tables_list:\n",
    "        head_tags=table.find_element_by_xpath('.//thead/tr')\n",
    "        child=head_tags.find_elements_by_xpath('//th')\n",
    "        date=head_tags.find_element_by_xpath('//th[.=\"Date (dd/mm/yyyy)\"]')\n",
    "        startup_name=head_tags.find_element_by_xpath('//th[.=\"Startup Name\"]')\n",
    "        industry_vertical=head_tags.find_element_by_xpath('//th[.=\"Industry / Vertical\"]')\n",
    "        sub_vertical=head_tags.find_element_by_xpath('//th[.=\"Sub-Vertical\"]')\n",
    "        \n",
    "        city_location=head_tags.find_element_by_xpath('//th[.=\"City / Location\"]')\n",
    "        investors_name=head_tags.find_element_by_xpath('//th[.=\"Investors\\' Name\"]')\n",
    "        investment_type=head_tags.find_element_by_xpath('//th[.=\"Investment Type\"]')\n",
    "        amount=head_tags.find_element_by_xpath('//th[.=\"Amount (In USD)\"]')\n",
    "        \n",
    "        tr_rows_list=table.find_elements_by_xpath('.//tbody/tr')\n",
    "        \n",
    "        for tr in tr_rows_list:\n",
    "            xpath_date='.//td[position()='+str(child.index(date)+1)+']'\n",
    "            xpath_startup_name='.//td[position()='+str(child.index(startup_name)+1)+']'\n",
    "            xpath_industry_vertical='.//td[position()='+str(child.index(industry_vertical)+1)+']'\n",
    "            xpath_sub_vertical='.//td[position()='+str(child.index(sub_vertical)+1)+']'\n",
    "            xpath_city_location='.//td[position()='+str(child.index(city_location)+1)+']'\n",
    "            xpath_investors_name='.//td[position()='+str(child.index(investors_name)+1)+']'\n",
    "            xpath_investment_type='.//td[position()='+str(child.index(investment_type)+1)+']'\n",
    "            xpath_amount='.//td[position()='+str(child.index(amount)+1)+']'\n",
    "            try:\n",
    "                date_list.append(tr.find_element_by_xpath(xpath_date).text)\n",
    "            except:\n",
    "                date_list.append('-')\n",
    "            try:\n",
    "                startup_list.append(tr.find_element_by_xpath(xpath_startup_name).text)\n",
    "            except:\n",
    "                startup_list.append('-')\n",
    "            try:\n",
    "                industry_vertical_list.append(tr.find_element_by_xpath(xpath_industry_vertical).text)\n",
    "            except:\n",
    "                industry_vertical_list.append('-')\n",
    "            try:\n",
    "                sub_vertical_list.append(tr.find_element_by_xpath(xpath_sub_vertical).text)\n",
    "            except:\n",
    "                sub_vertical_list.append('-')\n",
    "            try:\n",
    "                city_location_list.append(tr.find_element_by_xpath(xpath_city_location).text)\n",
    "            except:\n",
    "                city_location_list.append('-')\n",
    "            try:\n",
    "                investors_name_list.append(tr.find_element_by_xpath(xpath_investors_name).text)\n",
    "            except:\n",
    "                investors_name_list.append('-')\n",
    "            try:\n",
    "                investment_type_list.append(tr.find_element_by_xpath(xpath_investment_type).text)\n",
    "            except:\n",
    "                investment_type_list.append('-')\n",
    "            try:\n",
    "                amount_list.append(tr.find_element_by_xpath(xpath_amount).text)\n",
    "            except:\n",
    "                amount_list.append('-')\n",
    "    res_table=pd.DataFrame({'Date':date_list,'Startup Name':startup_list,'Industry / Vertical':industry_vertical_list,\n",
    "                           'Sub-Vertical':sub_vertical_list,'City / Location':city_location_list,\"Investors' Name\":investors_name_list,\n",
    "                           'Investment Type':investment_type_list,'Amount (In USD)':amount_list})\n",
    "        \n",
    "    return res_table\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394a6159",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_funding_deals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7beeb3a6",
   "metadata": {},
   "source": [
    "7. Write a program to scrap all the available details of best gaming laptops from digit.in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d389533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gaming_laptops():\n",
    "    #intiating the browser\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    \n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe',chrome_options=options)\n",
    "    #loading the given url\n",
    "    driver.get('https://www.digit.in/')\n",
    "    #Send input to search bar\n",
    "    time.sleep(5)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    url=driver.find_element_by_xpath('//a[.=\"Laptops\"]').get_attribute('href')\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    driver.find_element_by_xpath('//a[.=\"Best Gaming Laptops in India\"]').click()\n",
    "    time.sleep(3)\n",
    "    prod_list=[]\n",
    "    sell_list=[]\n",
    "    price_list=[]\n",
    "         \n",
    "    head_tr=driver.find_element_by_xpath('//table[@id=\"summtable\"]/thead/tr')\n",
    "    child=head_tr.find_elements_by_xpath('//th')\n",
    "    prod_name=head_tr.find_element_by_xpath('//th[.=\"Product Name\"]')\n",
    "    seller=head_tr.find_element_by_xpath('//th[.=\"Seller\"]')\n",
    "    price=head_tr.find_element_by_xpath('//th[.=\"Price\"]')\n",
    "\n",
    "    tr_rows=driver.find_elements_by_xpath('//table[@id=\"summtable\"]/tbody/tr')\n",
    "    for tr in tr_rows:\n",
    "        xpath_prod_name='.//td[position()='+str(child.index(prod_name)+1)+']'\n",
    "        xpath_seller='.//td[position()='+str(child.index(seller)+1)+']'\n",
    "        xpath_price='.//td[position()='+str(child.index(price)+1)+']'\n",
    "\n",
    "        try:\n",
    "            prod_list.append(tr.find_element_by_xpath(xpath_prod_name).text)\n",
    "        except:\n",
    "            prod_list.append('-')\n",
    "        try:\n",
    "            sell_list.append(tr.find_element_by_xpath(xpath_seller).text)\n",
    "        except:\n",
    "            sell_list.append('-')\n",
    "        try:\n",
    "            price_list.append(tr.find_element_by_xpath(xpath_price).text)\n",
    "        except:\n",
    "            price_list.append('-')\n",
    "    res_table=pd.DataFrame({'Product Name':prod_list,'Seller':sell_list,'Price':price_list})\n",
    "    return res_table\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_gaming_laptops()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8fa73",
   "metadata": {},
   "source": [
    "8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be\n",
    "scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a25f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_billionaires():\n",
    "    #intiating the browser\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    \n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe',chrome_options=options)\n",
    "    #loading the given url\n",
    "    driver.set_page_load_timeout(180)\n",
    "    driver.get('https://www.forbes.com/')\n",
    "    \n",
    "    #Send input to search bar\n",
    "    driver.maximize_window()\n",
    "    WebDriverWait(driver,60).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"footer__nav\"]/a[.=\"Billionaires\"]')))\n",
    "    driver.find_element_by_xpath('//div[@class=\"footer__nav\"]/a[.=\"Billionaires\"]').click()\n",
    "    WebDriverWait(driver,60).until(EC.presence_of_element_located((By.XPATH,'//a[@aria-label=\"The World\\'s Billionaires List 2021\"]')))\n",
    "    driver.find_element_by_xpath('//a[@aria-label=\"The World\\'s Billionaires List 2021\"]').click()\n",
    "    WebDriverWait(driver,60).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"table-row-group\"]/div[@class=\"table-row \" or @class=\"table-row expanded\"]')))\n",
    "    rank_list=[]\n",
    "    name_list=[]\n",
    "    net_worth_list=[]\n",
    "    age_list=[]\n",
    "    city_list=[]\n",
    "    source_list=[]\n",
    "    ind_list=[]\n",
    "    \n",
    "    list_rows=driver.find_elements_by_xpath('//div[@class=\"table-row-group\"]/div[@class=\"table-row \" or @class=\"table-row expanded\"]')\n",
    "    \n",
    "    for row in list_rows:\n",
    "        try:            \n",
    "            rank_list.append(row.find_element_by_xpath('.//div[@class=\"rank\"]').text.replace('.',''))\n",
    "        except:\n",
    "            rank_list.append('-')\n",
    "        try:\n",
    "            name_list.append(row.find_element_by_xpath('.//div[@class=\"personName\"]').text)\n",
    "        except:\n",
    "            name_list.append('-')\n",
    "        try:\n",
    "            net_worth_list.append(row.find_element_by_xpath('.//div[@class=\"netWorth\"]').text)\n",
    "        except:\n",
    "            net_worth_list.append('-')\n",
    "        try:\n",
    "            age_list.append(row.find_element_by_xpath('.//div[@class=\"age\"]').text)\n",
    "        except:\n",
    "            age_list.append('-')\n",
    "        try:\n",
    "            city_list.append(row.find_element_by_xpath('.//div[@class=\"countryOfCitizenship\"]').text)\n",
    "        except:\n",
    "            city_list.append('-')\n",
    "        try:\n",
    "            source_list.append(row.find_element_by_xpath('.//div[@class=\"source\"]').text)\n",
    "        except:\n",
    "            source_list.append('-')\n",
    "        try:\n",
    "            ind_list.append(row.find_element_by_xpath('.//div[@class=\"category\"]').text)\n",
    "        except:\n",
    "            ind_list.append('-')\n",
    "    res_table=pd.DataFrame({'Rank':rank_list,'Name':name_list,'Net worth':net_worth_list,'Age':age_list,'Citizenship':city_list,'Source':source_list,'Industry':ind_list})\n",
    "    return res_table\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6769f48f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_billionaires()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ec633b",
   "metadata": {},
   "source": [
    "9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted\n",
    "from any YouTube Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573fa98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_youtube_comments():\n",
    "    #intiating the browser\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    \n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe',chrome_options=options)\n",
    "    #loading the given url\n",
    "    driver.get('https://www.youtube.com/watch?v=LHBE6Q9XlzI')\n",
    "    driver.maximize_window()\n",
    "    time.sleep(10)\n",
    "    content_list=[]\n",
    "    up_vote_list=[]\n",
    "    time_list=[]\n",
    "    driver.execute_script(\"window.scrollBy(0,500)\",\"\")\n",
    "    i=0\n",
    "    while(i<100):\n",
    "        driver.execute_script(\"window.scrollBy(0,1500)\",\"\")\n",
    "        time.sleep(10)\n",
    "        i+=1\n",
    "        \n",
    "    comments_list=driver.find_elements_by_xpath('//div[@id=\"contents\"]/ytd-comment-thread-renderer[@class=\"style-scope ytd-item-section-renderer\"]')\n",
    "    for com in comments_list:\n",
    "        try:\n",
    "            content_list.append(com.find_element_by_xpath('.//yt-formatted-string[@id=\"content-text\"]').text)\n",
    "        except:\n",
    "            content_list.append('-')\n",
    "        try:\n",
    "            up_vote_list.append(com.find_element_by_xpath('.//span[@id=\"vote-count-middle\"]').text)\n",
    "        except:\n",
    "            up_vote_list.append('-')\n",
    "        try:\n",
    "            time_list.append(com.find_element_by_xpath('.//yt-formatted-string[@class=\"published-time-text above-comment style-scope ytd-comment-renderer\"]/a').text)\n",
    "        except:\n",
    "            time_list.append('-')\n",
    "    res=pd.DataFrame({'Comment':content_list,'UpVotes':up_vote_list,'Posted Time':time_list})\n",
    "    print(len(content_list))     \n",
    "    return res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed401ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_youtube_comments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e66341",
   "metadata": {},
   "source": [
    "10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in\n",
    "“London” location. You have to scrape hostel name, distance from city centre, ratings, total reviews,\n",
    "overall reviews, privates from price, dorms from price, facilities and property description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e0b936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hostel_info():\n",
    "    #intiating the browser\n",
    "    options = Options()\n",
    "    options.add_argument(\"--disable-notifications\")\n",
    "    \n",
    "    driver=webdriver.Chrome('C:/Users/yn/Desktop/Yuvi/DataTrained/seleniumWebdriver/chromedriver.exe',chrome_options=options)\n",
    "    #loading the given url\n",
    "    driver.get('https://www.hostelworld.com/')\n",
    "    driver.maximize_window()\n",
    "    WebDriverWait(driver,60).until(EC.presence_of_element_located((By.XPATH,'//input[@class=\"location-text\"]')))\n",
    "    driver.find_element_by_xpath('//a[@title=\"Hostels\"]').click()\n",
    "    time.sleep(5)\n",
    "    driver.find_element_by_xpath('//input[@id=\"home-search-keywords\"]').send_keys('London')\n",
    "    time.sleep(2)\n",
    "    driver.find_element_by_xpath('//ul[@class=\"suggestions\"]/li[@class=\"hover\"]').click()\n",
    "    driver.find_element_by_xpath('//button[.=\"Search\"]').click()\n",
    "    time.sleep(3)\n",
    "       \n",
    "    host_name=[]\n",
    "    dist_list=[]\n",
    "    rating_list=[]\n",
    "    t_reviews_list=[]\n",
    "    o_reviews_list=[]\n",
    "    privates_price_list=[]\n",
    "    dorms_price_list=[]\n",
    "    facilty_list=[]\n",
    "    desc_list=[]\n",
    "    is_next=True\n",
    "    WebDriverWait(driver,120).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"pagination-item pagination-next\"]')))\n",
    "   \n",
    "    while(is_next):\n",
    "        url_list=[]\n",
    "        WebDriverWait(driver,60).until(EC.presence_of_element_located((By.XPATH,'//div[@class=\"page-inner\"]//div[@class=\"prices-col\"]/ancestor::div[@class=\"property-card\"]')))\n",
    "        prop_list=driver.find_elements_by_xpath('//div[@class=\"page-inner\"]//div[@class=\"prices-col\"]/ancestor::div[@class=\"property-card\"]')\n",
    "        for prop in prop_list:\n",
    "            try:\n",
    "                host_name.append(prop.find_element_by_xpath('.//h2[1]').text)\n",
    "            except:\n",
    "                host_name.append('-')\n",
    "            try:\n",
    "                dist_list.append(prop.find_element_by_xpath('.//span[@class=\"description\"]').text)\n",
    "            except:\n",
    "                dist_list.append('-')\n",
    "            try:\n",
    "                rating_list.append(prop.find_element_by_xpath('.//div[contains(@class,\"score\")]').text)\n",
    "            except:\n",
    "                rating_list.append('-')\n",
    "            try:\n",
    "                t_reviews_list.append(prop.find_element_by_xpath('.//div[@class=\"reviews\"]').text)\n",
    "            except:\n",
    "                t_reviews_list.append('-')\n",
    "            try:\n",
    "                o_reviews_list.append(prop.find_element_by_xpath('.//div[@class=\"keyword\"]').text)\n",
    "            except:\n",
    "                o_reviews_list.append('-')\n",
    "            try:\n",
    "                privates_price_list.append(prop.find_element_by_xpath('.//p[.=\"Privates From\"]/following-sibling::div[1]').text)\n",
    "            except:\n",
    "                privates_price_list.append('-')\n",
    "            try:\n",
    "                dorms_price_list.append(prop.find_element_by_xpath('.//p[.=\"Dorms From\"]/following-sibling::div[1]').text)\n",
    "            except:\n",
    "                dorms_price_list.append('-')\n",
    "            try:\n",
    "                fac_info=''\n",
    "                facilty_list_item=prop.find_elements_by_xpath('.//div[@class=\"facilities-label facilities\"]/div')                \n",
    "                for item in facilty_list_item:\n",
    "                    fac_info=fac_info+', '+item.text\n",
    "                facilty_list.append(fac_info[2:])\n",
    "            except:\n",
    "                facilty_list.append('-')\n",
    "            try:            \n",
    "                url=prop.find_element_by_xpath('.//h2[1]/a').get_attribute('href')\n",
    "                url_list.append(url)               \n",
    "            except:\n",
    "                url_list.append('-')\n",
    "        driver.execute_script(\"window.open('');\")\n",
    "        driver.switch_to.window(driver.window_handles[1])\n",
    "        for prod in url_list:\n",
    "            try:\n",
    "                driver.get(prod)\n",
    "                time.sleep(4)\n",
    "                driver.find_element_by_xpath('//a[@class=\"toggle-content\"]').click()\n",
    "                time.sleep(2)\n",
    "                WebDriverWait(driver,30).until(EC.presence_of_element_located((By.XPATH,'(//div[@class=\"description-container\"])')))\n",
    "                if(len(driver.find_elements_by_xpath('//div[@class=\"description-container\"]'))>1):\n",
    "                    desc_list.append(prop.find_element_by_xpath('(//div[@class=\"description-container\"])[2]//div[@class=\"content\"]').text)\n",
    "                else:\n",
    "                    desc_list.append(prop.find_element_by_xpath('(//div[@class=\"initial-property-info\"])[1]').text)\n",
    "            except:\n",
    "                desc_list.append('-')\n",
    "        # Closing new_url tab\n",
    "        driver.close()\n",
    "        # Switching to old tab\n",
    "        driver.switch_to.window(driver.window_handles[0])       \n",
    "        try:\n",
    "            driver.refresh()\n",
    "            time.sleep(5)           \n",
    "            driver.find_element_by_xpath('//div[@class=\"pagination-item pagination-next\"]').click()\n",
    "        except NoSuchElementException:\n",
    "            is_next=False\n",
    "    res_table=pd.DataFrame({'Hostel Name':host_name,'Distance from city centre':dist_list,'Ratings':rating_list,'Total Reviews':t_reviews_list,'Overall review':o_reviews_list,\n",
    "                           'Privates from price':privates_price_list,'Dorms from price':dorms_price_list,'Facilities':facilty_list,'Property Description':desc_list})\n",
    "    return res_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86432270",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_hostel_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99046f37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
